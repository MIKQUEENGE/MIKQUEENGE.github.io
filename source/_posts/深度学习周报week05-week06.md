---
title: 深度学习周报week05-week06
toc: true
date: 2018-05-12 10:22:12
categories:
- deep learning
tags:
- BN
---

## 过拟合

过拟合，就是拟合函数由于顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。

<!-- more -->

## 正则化

[参考链接](https://blog.csdn.net/u012162613/article/details/44261657)

当训练数据不够多或训练过度时，常常会导致过拟合。正则化就是避免过拟合的一个办法。

<u>正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</u>

### L2正则化（权重衰减）

L2正则化就是在代价函数后面再加上一个正则化项：

![](/images/L2.jpg)

### L1正则化

L1正则化就是在原始的代价函数后面加上一个L1正则化项，即所有权重w的绝对值的和，乘以λ/n：

![](/images/L1.jpg)

## Batch Normalization(BN)

在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。

![](/images/BN.png)

训练过程中采用batch 随机梯度下降，其中E(xk)指的是每一批训练数据神经元xk的平均值；分母指的是每一批数据神经元xk激活度的一个标准差。

引入了可学习参数γ、β：

![](/images/BN_y.png)

因此Batch Normalization网络层的前向传导过程公式为：

![](/images/BN_back.png)

其中m为mini-batch size。

### 在CNN中的使用

由于BN是对单个神经元的运算，因此为了避免参数过多的情况，使用类似权值共享的策略，把一整张特征图当作一个神经元处理。

## cifar 10

继续调整网络结构和参数。

## 项目准备

了解频谱图以及音频采样。

